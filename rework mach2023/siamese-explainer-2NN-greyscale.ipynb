{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a2880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Dense, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13e2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperopt_emb():\n",
    "    inputs = Input(sh)\n",
    "\n",
    "    x = Conv2D(64, (5, 5), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(32, (5, 5), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(12, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Flatten layer\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    encoder = Model(inputs, x, name=\"embedding\")\n",
    "    # encoder.summary()\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def get_siamese_net():\n",
    "    encoder = get_hyperopt_emb() # get encoder sub-networkk\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(sh)\n",
    "    right_input = Input(sh)\n",
    "\n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = encoder(left_input)\n",
    "    encoded_r = encoder(right_input)\n",
    "\n",
    "    dist_layer = Lambda(lambda tensors: K.abs(tensors[0]-tensors[1])) #l1\n",
    "    # dist_layer = Lambda(lambda tensors: K.square(tensors[0]-tensors[1])) #l2\n",
    "\n",
    "    # Merge layer\n",
    "    inputs_merged = dist_layer([encoded_l, encoded_r])\n",
    "\n",
    "    # Final layer\n",
    "    prediction = Dense(1, activation='sigmoid', name='preds')(inputs_merged)\n",
    "\n",
    "    # Connect the inputs with the outputs\n",
    "    model = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcced8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# get k random element from a list \n",
    "def get_n_rnd_element_from_list(lst, n):\n",
    "    return random.sample(lst, k=n)\n",
    "\n",
    "# get k random element from a list excluding the exception parameter value from the list\n",
    "def get_n_rnd_element_from_list_excluding_parameter(lst, exception, n):\n",
    "    possible_choices = [v for v in lst if v != exception]\n",
    "    return get_n_rnd_element_from_list(possible_choices, n)\n",
    "\n",
    "def add_channel_dimension(img):\n",
    "    # Add a `channels` dimension, so that the spectrogram can be used\n",
    "    # as image-like input data with convolution layers (which expect\n",
    "    # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "    return img[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8b3e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'images'\n",
    "val_classes = ['Atlantean', 'Anglo-Saxon_Futhorc', 'Greek', 'Japanese_(katakana)', 'Sylheti'] \n",
    "test_classes = ['Hebrew', 'Tengwar', 'Latin', 'Oriya', 'Sanskrit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6e430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = (105, 105, 1)\n",
    "cp = 'character' # character placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43003bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 23:30:21.630047: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-27 23:30:21.630140: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_net()\n",
    "model.load_weights('models/siamese_net_1ch5kiter.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26641aeb",
   "metadata": {},
   "source": [
    "Val/Test accuracy performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "088ce8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot_validate(model, alphabets, C, src, runs_per_alphabet = 300):\n",
    "    print('Making ' + str(runs_per_alphabet) + ' random tasks (per alphabet) on ' + str(C) + '-Way One-Shot support sets...')\n",
    "    mean_global_accuracy = 0\n",
    "\n",
    "    for alphabet in alphabets:\n",
    "        mean_alphabet_accuracy = 0\n",
    "\n",
    "        for _ in range(runs_per_alphabet):\n",
    "            X, y, c = get_one_shot_batch(alphabet, alphabets, C, src)\n",
    "            probabilities = model.predict_on_batch([X[:, 0], X[:, 1]])\n",
    "\n",
    "            if len(set(probabilities.flatten())) == 1 and len(probabilities) != 1:\n",
    "                # logging.info(probabilities.flatten())\n",
    "                print(\"All probabilities are equal, setting accuracy to 0.0\")\n",
    "                accuracy = 0.0\n",
    "            elif np.argmax(probabilities) == np.argmax(y):\n",
    "                accuracy = 1.0\n",
    "            else:\n",
    "                accuracy = 0.0\n",
    "\n",
    "            mean_alphabet_accuracy += accuracy\n",
    "            mean_global_accuracy += accuracy\n",
    "        \n",
    "        mean_alphabet_accuracy /= runs_per_alphabet\n",
    "        print(alphabet + ' Alphabet' + ', accuracy: ' + str(mean_alphabet_accuracy))\n",
    "    \n",
    "    mean_global_accuracy /= (len(alphabets) * runs_per_alphabet)\n",
    "    print('Mean global accuracy: ' + str(mean_global_accuracy))\n",
    "    print('----------------------------------------------------------------')\n",
    "    return mean_global_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "188446de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a C-way 1-shot batch\n",
    "# set: set of all validation/test classes (singular characters)\n",
    "# query_class: query class\n",
    "# C: C-way (C is tipically the length of all classes set)\n",
    "def get_one_shot_batch(query_aplhabet, alphabets, C, src):\n",
    "    S, X, y, c = [], [], [], []\n",
    "\n",
    "    # select a random character and then a random sample of that character as query sample\n",
    "    rnd_char = get_n_rnd_element_from_list(os.listdir(os.path.join(src,query_aplhabet)), 1)[0] # select a random character of the current alphabet\n",
    "    charsamples = os.listdir(os.path.join(src, query_aplhabet, rnd_char)) # get char samples of selected character\n",
    "    query_sample = get_n_rnd_element_from_list(charsamples, 1)[0] # select a random samples of the current character of the current alphabet\n",
    "    # query_img = cv2.imread(os.path.join(src, query_aplhabet, rnd_char, query_sample))\n",
    "    query_img = cv2.cvtColor(cv2.imread(os.path.join(src, query_aplhabet, rnd_char, query_sample)), cv2.COLOR_BGR2GRAY)\n",
    "    query_img = add_channel_dimension(query_img)\n",
    "    \n",
    "    # positive sample pair\n",
    "    pos_sample = get_n_rnd_element_from_list_excluding_parameter(charsamples, query_sample, 1)[0] # get a different char of the same aplha as positive sample\n",
    "    # pos_img = cv2.imread(os.path.join(src, query_aplhabet, rnd_char, pos_sample))\n",
    "    pos_img = cv2.cvtColor(cv2.imread(os.path.join(src, query_aplhabet, rnd_char, pos_sample)), cv2.COLOR_BGR2GRAY)\n",
    "    pos_img = add_channel_dimension(pos_img)\n",
    "\n",
    "    X.append([query_img, pos_img])\n",
    "    y.append(1)\n",
    "    S.append(pos_img)\n",
    "    c.append([query_aplhabet + \":\" + rnd_char.replace(cp, \"\"), query_aplhabet + \":\" + rnd_char.replace(cp, \"\")])\n",
    "\n",
    "    # negative pairs (un carattere per ognuno degli alfabeti)\n",
    "    negative_aplhabets = get_n_rnd_element_from_list_excluding_parameter(alphabets, query_aplhabet, 4) # get all alphabets different from the query input \n",
    "    for neg_alpha in negative_aplhabets:\n",
    "        negative_chars = os.listdir(os.path.join(src,neg_alpha)) # get all character folders\n",
    "        neg_char = get_n_rnd_element_from_list(negative_chars, 1)[0] # select a random character of the current alphabet\n",
    "        neg_samples = os.listdir(os.path.join(src, neg_alpha, neg_char)) # get char samples of selected character\n",
    "        neg_sample = get_n_rnd_element_from_list(neg_samples, 1)[0] # select a random samples of the current character of the current alphabet\n",
    "        # neg_img = cv2.imread(os.path.join(src, neg_alpha, neg_char, neg_sample))\n",
    "        neg_img = cv2.cvtColor(cv2.imread(os.path.join(src, neg_alpha, neg_char, neg_sample)), cv2.COLOR_BGR2GRAY)\n",
    "        neg_img = add_channel_dimension(neg_img)\n",
    "\n",
    "        X.append([query_img, neg_img])\n",
    "        y.append(0)\n",
    "        S.append(neg_img)\n",
    "        c.append([query_aplhabet + \":\" + rnd_char.replace(cp, \"\"), neg_alpha + \":\" + neg_char.replace(cp, \"\")])\n",
    "\n",
    "    \n",
    "\n",
    "    return np.array(X), np.array(y), np.array(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef79123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validate on evaluation alphabets...\")\n",
    "one_shot_validate(model, val_classes, len(val_classes), src)\n",
    "print(\"Validate on test alphabets...\")\n",
    "one_shot_validate(model, test_classes, len(test_classes), src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bb2a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a C-way 1-shot batch\n",
    "# set: set of all validation/test classes (singular characters)\n",
    "# query_class: query class\n",
    "# C: C-way (C is tipically the length of all classes set)\n",
    "def get_one_shot_batch(query_aplhabet, alphabets, C, src):\n",
    "    S, X, y, c = [], [], [], []\n",
    "\n",
    "    # select a random character and then a random sample of that character as query sample\n",
    "    rnd_char = get_n_rnd_element_from_list(os.listdir(os.path.join(src,query_aplhabet)), 1)[0] # select a random character of the current alphabet\n",
    "    charsamples = os.listdir(os.path.join(src, query_aplhabet, rnd_char)) # get char samples of selected character\n",
    "    query_sample = get_n_rnd_element_from_list(charsamples, 1)[0] # select a random samples of the current character of the current alphabet\n",
    "    # query_img = cv2.imread(os.path.join(src, query_aplhabet, rnd_char, query_sample))\n",
    "    query_img = cv2.cvtColor(cv2.imread(os.path.join(src, query_aplhabet, rnd_char, query_sample)), cv2.COLOR_BGR2GRAY)\n",
    "    query_img = add_channel_dimension(query_img)\n",
    "    \n",
    "    # positive sample pair\n",
    "    pos_sample = get_n_rnd_element_from_list_excluding_parameter(charsamples, query_sample, 1)[0] # get a different char of the same aplha as positive sample\n",
    "    # pos_img = cv2.imread(os.path.join(src, query_aplhabet, rnd_char, pos_sample))\n",
    "    pos_img = cv2.cvtColor(cv2.imread(os.path.join(src, query_aplhabet, rnd_char, pos_sample)), cv2.COLOR_BGR2GRAY)\n",
    "    pos_img = add_channel_dimension(pos_img)\n",
    "\n",
    "    X.append([query_img, pos_img])\n",
    "    y.append(1)\n",
    "    S.append(pos_img)\n",
    "    c.append([query_aplhabet + \":\" + rnd_char.replace(cp, \"\"), query_aplhabet + \":\" + rnd_char.replace(cp, \"\")])\n",
    "\n",
    "    # negative pairs (un carattere per ognuno degli alfabeti)\n",
    "    negative_aplhabets = get_n_rnd_element_from_list_excluding_parameter(alphabets, query_aplhabet, 4) # get all alphabets different from the query input \n",
    "    for neg_alpha in negative_aplhabets:\n",
    "        negative_chars = os.listdir(os.path.join(src,neg_alpha)) # get all character folders\n",
    "        neg_char = get_n_rnd_element_from_list(negative_chars, 1)[0] # select a random character of the current alphabet\n",
    "        neg_samples = os.listdir(os.path.join(src, neg_alpha, neg_char)) # get char samples of selected character\n",
    "        neg_sample = get_n_rnd_element_from_list(neg_samples, 1)[0] # select a random samples of the current character of the current alphabet\n",
    "        # neg_img = cv2.imread(os.path.join(src, neg_alpha, neg_char, neg_sample))\n",
    "        neg_img = cv2.cvtColor(cv2.imread(os.path.join(src, neg_alpha, neg_char, neg_sample)), cv2.COLOR_BGR2GRAY)\n",
    "        neg_img = add_channel_dimension(neg_img)\n",
    "\n",
    "        X.append([query_img, neg_img])\n",
    "        y.append(0)\n",
    "        S.append(neg_img)\n",
    "        c.append([query_aplhabet + \":\" + rnd_char.replace(cp, \"\"), neg_alpha + \":\" + neg_char.replace(cp, \"\")])\n",
    "\n",
    "    \n",
    "\n",
    "    return query_img, np.array(S), np.array(X), np.array(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d091e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dimension(s):\n",
    "    ''' Expand the spectrogram dimensions for tensorflow predict_on_batch function use \n",
    "        input s: spectrogram '''\n",
    "    return np.expand_dims(copy.deepcopy(s), axis=0)\n",
    "\n",
    "def predict_similarity(f, x, s):\n",
    "    return f.predict_on_batch([expand_dimension(x), expand_dimension(s)])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19010df8",
   "metadata": {},
   "source": [
    "Inizializzo in maniera simile a quello che faccio con SINEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69ce765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology\n",
    "from skimage.filters import sobel\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.segmentation import felzenszwalb, slic, quickshift, watershed\n",
    "from skimage.segmentation import mark_boundaries, find_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f5a1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_image(img):\n",
    "    return np.stack((img[:,:,0],)*3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a2bdac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    ni = None\n",
    "    \n",
    "    if len(img.shape) == 2:\n",
    "        # Two dimensions found. Adding fake channel and stacking it in each RGB dimension.\n",
    "        ni = add_channel_dimension(img)\n",
    "        ni = stack_image(ni)\n",
    "    elif img.shape[2] != 3:\n",
    "        # Third dimension does not have 3 channels. Stacking the first one in each RGB dimension.\n",
    "        ni = stack_image(img)\n",
    "    \n",
    "    return ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df6af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "683403de",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6712f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "shapelike_cmap = LinearSegmentedColormap.from_list('shaplike_gradient', (\n",
    "    # Edit this gradient at https://eltos.github.io/gradient/#1E88E5-91C5F2-FFFFFF-FF6395-FF0052\n",
    "    (0.000, (0.118, 0.533, 0.898)),\n",
    "    (0.250, (0.569, 0.773, 0.949)),\n",
    "    (0.500, (1.000, 1.000, 1.000)),\n",
    "    (0.750, (1.000, 0.388, 0.584)),\n",
    "    (1.000, (1.000, 0.000, 0.322)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "defca0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams.update({'font.size': 17})\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47cddf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_script(n, label='s'):\n",
    "    if n == 1:\n",
    "        return label + '\\N{SUBSCRIPT ONE}'\n",
    "    elif n == 2:\n",
    "        return label + '\\N{SUBSCRIPT TWO}'\n",
    "    elif n == 3:\n",
    "        return label + '\\N{SUBSCRIPT THREE}'\n",
    "    elif n == 4:\n",
    "        return label + '\\N{SUBSCRIPT FOUR}'\n",
    "    elif n == 5:\n",
    "        return label + '\\N{SUBSCRIPT FIVE}'\n",
    "    else:\n",
    "        return 'null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f99fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explanation_omniglet(x, S, labels, norm_attributions, probabilities, cmap):\n",
    "    N = len(norm_attributions)\n",
    "    \n",
    "    plt.figure()\n",
    "    f, axarr = plt.subplots(2, N + 1,figsize=(30,10))\n",
    "\n",
    "    axarr[1, 0].imshow(x, cmap='gray')\n",
    "    axarr[1, 0].set_axis_off()\n",
    "    axarr[1, 0].set_title('Query x;\\n y = ' + str(labels[0][0]))\n",
    "\n",
    "    for i in range(N):\n",
    "        sim = round(probabilities[i][0], 2)\n",
    "    \n",
    "        # THIRD ROW: Normalized attributions\n",
    "        im3 = axarr[0, i+1].imshow(norm_attributions[i], cmap='coolwarm', interpolation='none', vmin=-1, vmax=1)\n",
    "#         plt.colorbar(im3, ax=axarr[0, i+1])\n",
    "        axarr[0, i+1].set_axis_off()\n",
    "        subscripth = sub_script(i+1, 'h')\n",
    "        axarr[0, i+1].set_title(subscripth + '; y = ' + str(labels[i][1]))\n",
    "        \n",
    "        \n",
    "        # 4th ROW: Original Inputs\n",
    "        axarr[1, i+1].imshow(S[i], cmap='gray')\n",
    "        axarr[1, i+1].set_axis_off()\n",
    "        subscripts = 's' + str(i+1)\n",
    "        axarr[1, i+1].set_title(subscripts + \"; sim = \" + str(sim)[-2:])\n",
    "\n",
    "    # Empty 2 row 1 column element\n",
    "    axarr[0, 0].set_axis_off()\n",
    "    axarr[0, 0].text(0.0, 0.5, '')\n",
    "    \n",
    "    \n",
    "    f.subplots_adjust(right=0.95)  # making some room for cbar\n",
    "    # getting the lower left (x0,y0) and upper right (x1,y1) corners:\n",
    "    [[x00,y00],[x01,y01]] = axarr[0, N].get_position().get_points()\n",
    "    [[x10,y10],[x11,y11]] = axarr[1, N].get_position().get_points()\n",
    "    \n",
    "    pad = 0.01; width = 0.005\n",
    "    cbar_ax = f.add_axes([x11+pad, y10, width, y01-y10])\n",
    "    axcb = f.colorbar(im3, cax=cbar_ax)\n",
    "    # axcb = f.colorbar(im3, cax=cbar_ax, ticks=[-1, 0, 1])\n",
    "    # axcb.ax.set_yticklabels([str(max_neg), '0', str(max_pos)])  # vertically oriented colorbar\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846fd17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "314ee350",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_required = 1 # da inserire in qualche parte in sinex quando si inizializza, dipende dal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee426683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_input(img_to_segment):\n",
    "    if stack_required:\n",
    "        img_to_segment = preprocess_img(img_to_segment)\n",
    "        \n",
    "    if sobel_required: # Watershed\n",
    "        img_to_segment = sobel(rgb2gray(img_to_segment))\n",
    "        \n",
    "    if mask_required: # MaskSLIC\n",
    "        ar = morphology.remove_small_objects(ar = rgb2gray(img_to_segment) < 0.5, min_size = 200)\n",
    "        mask = morphology.remove_small_holes(ar = ar, area_threshold = 500)\n",
    "        mask = morphology.opening(mask)\n",
    "        aparams['mask'] = mask\n",
    "\n",
    "    R = algo(image=img_to_segment, **aparams) # creating segments on support set si input\n",
    "#     nR = np.unique(R).shape[0] # number of segments\n",
    "    uR = np.unique(R) # unique segments\n",
    "    \n",
    "    return R, uR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4b8188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # returns only the segments' index containing an actual drawn of the written character \n",
    "def find_drawn_segments(img, segments):\n",
    "    dsid = [] # drawn segments indexes \n",
    "    segs = np.unique(segments).tolist() # unique segments as index list\n",
    "    \n",
    "    for seg in segs:\n",
    "        pixels = np.where(segments==seg)\n",
    "        mean = np.mean(img[pixels])\n",
    "\n",
    "        # debugging purposes\n",
    "        if np.isnan(mean) == True: print(\"Segment: \" + str(seg) + \" mean is nan.\")\n",
    "\n",
    "        # restrict only on written areas\n",
    "        if mean != 255:\n",
    "            dsid.append(seg)\n",
    "        \n",
    "    return dsid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fd02f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_segment_drawn(img, seg, segments):\n",
    "    pixels = np.where(segments==seg)\n",
    "    mean = np.mean(img[pixels])\n",
    "    \n",
    "    return True if mean != 255 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1302f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METODO DIVERSO RISPETTO LO STATO ATTUALE DI SINEX, bisogna vedere se si può uniformare allo stato attuale di sinex\n",
    "def perturb_image_by_disabling(img, segments, segments_to_disable, replace_value = 0):\n",
    "    wc = copy.deepcopy(img) # makes a working deep copy of the image \n",
    "\n",
    "    for seg in segments_to_disable:\n",
    "        pixels = np.where(segments==seg) # finding segment's pixel coordinate\n",
    "        wc[pixels] = replace_value # replacing value\n",
    "    \n",
    "    return wc # sto restituendo solo l'ultimo pixels: WARNING! al momento questo metovo viene usato\n",
    "    # passando solo 1 segmento come parametro\n",
    "    \n",
    "  #  return wc, np.where(wc == replace_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1de667b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_image_by_enabling(empty_img, segments, segments_to_enable, replace_value = 0):\n",
    "    wc = copy.deepcopy(empty_img) # makes a working deep copy of the image \n",
    "\n",
    "    for seg in segments_to_enable:\n",
    "        pixels = np.where(segments==seg) # finding segment's pixel coordinate\n",
    "        wc[pixels] = replace_value # replacing value\n",
    "\n",
    "    # return wc, pixels\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2f4f3",
   "metadata": {},
   "source": [
    "# SINEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dccc923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_sinex(f, x, S, verbose=False):\n",
    "    E = []\n",
    "    C = {} # { sidx: {'segments': [...], 'deltas': []} } contribution values map\n",
    "    \n",
    "    for sidx in range(len(S)):\n",
    "        si = S[sidx] # support set si\n",
    "        v = predict_similarity(f, x, si) # calculate initial similarity\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Analyzig support set index:\", sidx, \"Predicted similarity:\", v)\n",
    "\n",
    "        R, uR = segment_input(si.copy()) # creating segments on support set si input\n",
    "#         plt.imshow(mark_boundaries(preprocess_img(si), R))\n",
    "#         plt.show()\n",
    "    \n",
    "        # Initializes current sample contribution\n",
    "        hi = np.full(sh, float(\"-inf\"))\n",
    "        \n",
    "        if verbose:\n",
    "            plt.imshow(mark_boundaries(preprocess_img(si), R))\n",
    "            plt.show()\n",
    "        \n",
    "        C[sidx] = {} # segment -> delta map init\n",
    "        C[sidx]['R'], C[sidx]['contr_seg_map'] = R, []\n",
    "        \n",
    "#         print(\"Total segments:\", len(uR))\n",
    "#         dsidx = find_drawn_segments(si, R) # drawn segments' ids\n",
    "#         print(\"Total drawn segments:\", len(dsidx))\n",
    "        \n",
    "        # REMOVAL FORZATO PER MASKSLIC segmnento 0 è la maschera (o la somma di tutti i segm)\n",
    "        uR = np.delete(uR, np.where(uR == 0))\n",
    "\n",
    "        for seg in uR:\n",
    "            if is_segment_drawn(si, seg, R): # turning on and off only drawn segments\n",
    "                if mode == 1:\n",
    "                    zi = perturb_image_by_enabling(np.full(sh, 255), R, [seg], 0)\n",
    "                else:\n",
    "                    # disabilitare solo current seg \n",
    "                    zi = perturb_image_by_disabling(si, R, [seg], 255)\n",
    "                    \n",
    "                idxs = np.where(R == seg)\n",
    "                pxl = si[idxs]\n",
    "                \n",
    "                u = predict_similarity(f, x, zi) # calculate new similarity\n",
    "                d = v - u # calculate delta of similarity scores\n",
    "                c = d / len(pxl) \n",
    "                hi[idxs] = c\n",
    "        \n",
    "                if verbose:\n",
    "                    print(\"New sim:\", u)\n",
    "                    print(\"Delta:\", d)\n",
    "                    print(\"Segment:\", seg)\n",
    "                    print(\"Len:\", len(pxl))\n",
    "                    plt.imshow(zi, cmap='gray')\n",
    "                    plt.show()\n",
    "                    print(\"-------------------\")\n",
    "                \n",
    "                # append contribution value of the current segment \n",
    "                C[sidx]['contr_seg_map'].append((c, seg))\n",
    "                \n",
    "        # Appending current support set attribution's vector\n",
    "        E.append(hi)\n",
    "    \n",
    "    return E, C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81bbc11",
   "metadata": {},
   "source": [
    "# SINEXC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b4a4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments_to_disable_sinexc(curr_seg, drawn_segments):\n",
    "    drawn = drawn_segments.copy()\n",
    "    drawn.remove(0) # REMOVAL FORZATO PER MASKSLIC -> seg 0 è la maschera (o la somma di tutit i sgem)\n",
    "    \n",
    "    \n",
    "    total_segs_to_disable = round((len(drawn) * percentage_of_segments_to_disable) / 100)\n",
    "    \n",
    "#     print(\"Total sample to disable\", total_segs_to_disable)\n",
    "#     print(\"Total drawn segments\", drawn)\n",
    "    \n",
    "    if total_segs_to_disable <= 1:\n",
    "#         lo forzo ad averne 2, in modo tale da sceglierne poi solo 1 oltre il curr seg\n",
    "        total_segs_to_disable = 2\n",
    "\n",
    "    random_sample = random.sample(drawn, total_segs_to_disable - 1)\n",
    "    random_sample.append(curr_seg)\n",
    "    \n",
    "    return random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f449834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_sinexc(f, x, S, verbose=False):\n",
    "    E = []\n",
    "    C = {}\n",
    "    \n",
    "    for sidx in range(len(S)):\n",
    "        si = S[sidx] # support set si\n",
    "\n",
    "        v = predict_similarity(f, x, si) # calculate initial similarity\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Analyzig support set index:\", sidx, \"Predicted similarity:\", v)\n",
    "\n",
    "        R, uR = segment_input(si.copy()) # creating segments on support set si input\n",
    "        dsidx = find_drawn_segments(si, R) # drawn segments' ids\n",
    "        \n",
    "        # Initializes current sample contribution\n",
    "#         hi = np.zeros(sh)\n",
    "        hi = np.full(sh, float(\"-inf\"))\n",
    "        \n",
    "        C[sidx] = {} # segment -> delta map init\n",
    "        C[sidx]['R'], C[sidx]['contr_seg_map'] = R, []\n",
    "        \n",
    "        # REMOVAL FORZATO PER MASKSLIC segmnento 0 è la maschera (o la somma di tutti i segm)\n",
    "        uR = np.delete(uR, np.where(uR == 0))\n",
    "\n",
    "        for seg in uR:\n",
    "            if is_segment_drawn(si, seg, R): \n",
    "#                 _, segidx = perturb_image_by_disabling(si, R, [seg], 255) # seg parameter is the one we want to turn off\n",
    "#                 pixels = _[segidx]\n",
    "                sims = 0\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"Segment in analysis:\", seg)\n",
    "                    plt.imshow(_, cmap='gray')\n",
    "                    plt.show()\n",
    "                \n",
    "                for p in range(P):\n",
    "                    segs_to_disable = get_segments_to_disable_sinexc(seg, dsidx)\n",
    "                                        \n",
    "                    if mode == 1: # curr seg deve rimanere attivo insieme ad un tot di altri\n",
    "                        segs_to_enable = segs_to_disable.copy() # in questo caso sono gli unici che devono rimanere attivi\n",
    "                        zi = perturb_image_by_enabling(np.full(sh, 255), R, segs_to_enable, 0)\n",
    "                    else: # curr seg deve essere eliminato insieme ad un tot di altri\n",
    "                        zi = perturb_image_by_disabling(si, R, segs_to_disable, 255) # seg parameter is the one we want to turn off    \n",
    "                    \n",
    "                    u = predict_similarity(f, x, zi) # calculate new similarity\n",
    "                    sims += u\n",
    "        \n",
    "#                     if verbose:\n",
    "#                         print(\"Perturbation:\", p, \" for segment:\", seg)\n",
    "#                         plt.imshow(zi, cmap='gray')\n",
    "#                         plt.show()\n",
    "\n",
    "\n",
    "                segidx = np.where(R == seg) # qui uso i segidx e i pixels del segmento corrente in analisi\n",
    "                pixels = si[segidx]\n",
    "                \n",
    "                # qui uso i segidx e i pixels del segmento corrente in analisi\n",
    "                d = v - (sims / P) # calculating delta of similarity scores\n",
    "                c = d / len(pixels)\n",
    "                hi[segidx] = c\n",
    "                \n",
    "                # append contribution value of the current segment \n",
    "                C[sidx]['contr_seg_map'].append((c, seg))\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"New sim:\", u)\n",
    "                    print(\"Delta:\", d)\n",
    "                    print(\"Segment:\", seg)\n",
    "                    plt.imshow(zi, cmap='gray')\n",
    "                    plt.show()\n",
    "                    print(\"-------------------\")\n",
    "    \n",
    "        # Appending current support set attribution's vector\n",
    "        E.append(hi)\n",
    "        \n",
    "    return E, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed912327",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268db2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cab14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e69f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835349df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdceceb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89daa3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
