{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acbbbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Dense, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6815e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- EMBEDDING NETWORKS --- ##\n",
    "def get_embedding_network(shape):\n",
    "    inputs = tf.keras.layers.Input(shape)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    # Flatten layer / Global Max Pooling\n",
    "    # x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
    "    encoder = tf.keras.Model(inputs, x, name=\"embedding\")\n",
    "    # encoder.summary()\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c609f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet_siamese_net(img_shape):\n",
    "    encoder = get_embedding_network(img_shape)\n",
    "\n",
    "    anchor_input = tf.keras.layers.Input(name=\"anchor\", shape=img_shape)\n",
    "    positive_input = tf.keras.layers.Input(name=\"positive\", shape=img_shape)\n",
    "    negative_input = tf.keras.layers.Input(name=\"negative\", shape=img_shape)\n",
    "\n",
    "    anchor_encoded = encoder(anchor_input)\n",
    "    positive_encoded = encoder(positive_input)\n",
    "    negative_encoded = encoder(negative_input)\n",
    "\n",
    "    # Distance layer\n",
    "    dist_layer = tf.keras.layers.Lambda(triplet_pairs_distance, name = 'distance_layer')  \n",
    "    \n",
    "    # Output layer\n",
    "    distances = dist_layer([anchor_encoded, positive_encoded, negative_encoded])\n",
    "\n",
    "    # Connect the inputs with the outputs\n",
    "    model = tf.keras.Model(inputs=[anchor_input, positive_input, negative_input], outputs=distances)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3e2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet ap_distance, an_distance\n",
    "def triplet_pairs_distance(vectors):\n",
    "    (anchor, positive, negative) = vectors\n",
    "    ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "    an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "    return (ap_distance, an_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a8151",
   "metadata": {},
   "source": [
    "### le 2 funzioni di get siamese net, get embedding sono diverse. triplet_pairs_distance non ce l'ho nella 2SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda203a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# get k random element from a list \n",
    "def get_n_rnd_element_from_list(lst, n):\n",
    "#     return random.choices(lst, k=n)\n",
    "    return random.sample(lst, k=n)\n",
    "\n",
    "# get k random element from a list excluding the exception parameter value from the list\n",
    "def get_n_rnd_element_from_list_excluding_parameter(lst, exception, n):\n",
    "    possible_choices = [v for v in lst if v != exception]\n",
    "    return get_n_rnd_element_from_list(possible_choices, n)\n",
    "\n",
    "def add_channel_dimension(img):\n",
    "    # Add a `channels` dimension, so that the spectrogram can be used\n",
    "    # as image-like input data with convolution layers (which expect\n",
    "    # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "    return img[..., tf.newaxis]\n",
    "\n",
    "\n",
    "#### VARIANTE ####\n",
    "def read_image(path, img_type):\n",
    "    img = cv2.imread(path)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB if img_type == 'RGB' else cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4088c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'images'\n",
    "val_classes = ['162.Canada_Warbler', '050.Eared_Grebe', '140.Summer_Tanager', '195.Carolina_Wren', '176.Prairie_Warbler']\n",
    "test_classes = ['057.Rose_breasted_Grosbeak', '047.American_Goldfinch', '094.White_breasted_Nuthatch', '143.Caspian_Tern', '038.Great_Crested_Flycatcher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef71610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8111690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 16:46:36.677396: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-27 16:46:36.677835: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = get_triplet_siamese_net(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d864ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('siamese_net.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47acbbcb",
   "metadata": {},
   "source": [
    "Val/test accuracy performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "167561d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot_validate_triplet(model, classes, C, src, runs_per_class, img_type):\n",
    "    print('Making ' + str(runs_per_class) + ' random tasks (per class) on ' + str(C) + '-Way One-Shot support sets...')\n",
    "    mean_global_accuracy = 0\n",
    "\n",
    "    for cla in classes:\n",
    "        mean_class_accuracy = 0\n",
    "\n",
    "        for _ in range(runs_per_class):\n",
    "            anchor_img, positive_img, negatives, labels = get_one_shot_batch_validate_triplet(cla, classes, C, src, img_type)\n",
    "\n",
    "            distances = []\n",
    "            for i in range(len(negatives)):\n",
    "                X = [[anchor_img, positive_img, negatives[i]]]\n",
    "                X = np.array(X)\n",
    "                ap_distance, an_distance = model([X[:,0], X[:,1], X[:,2]])\n",
    "\n",
    "                if i == 0:\n",
    "                    distances.append(ap_distance.numpy()[0]) # ap_distance la salvo come prima e soltanto una volta\n",
    "\n",
    "                distances.append(an_distance.numpy()[0])\n",
    "\n",
    "            if np.argmin(distances) == 0: # the smallest distance must be the first one (anchor-positive pair)\n",
    "                accuracy = 1.0\n",
    "            else:\n",
    "                accuracy = 0.0\n",
    "\n",
    "            mean_class_accuracy += accuracy\n",
    "            mean_global_accuracy += accuracy\n",
    "\n",
    "        mean_class_accuracy /= runs_per_class\n",
    "        print(cla + ' Class' + ', accuracy: ' + str(mean_class_accuracy))\n",
    "\n",
    "    mean_global_accuracy /= (len(classes) * runs_per_class)\n",
    "    print('Mean global accuracy: ' + str(mean_global_accuracy))\n",
    "    print('----------------------------------------------------------------')\n",
    "    \n",
    "    return mean_global_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "891b6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_shot_batch_validate_triplet(anchor_class, classes, C, src, img_type):\n",
    "    negatives, labels = [], []\n",
    "    \n",
    "    anchor_samples = get_n_rnd_element_from_list(os.listdir(os.path.join(src, anchor_class)), 2) # select anchor, positive\n",
    "    \n",
    "    # anchor, positive samplese\n",
    "    anchor_sample, positive_sample = anchor_samples[0], anchor_samples[1]\n",
    "    anchor_img = read_image(os.path.join(src, anchor_class, anchor_sample), img_type) \n",
    "    positive_img = read_image(os.path.join(src, anchor_class, positive_sample), img_type) \n",
    "\n",
    "    labels.append(anchor_class)\n",
    "\n",
    "    # negative pair samples\n",
    "    negative_classes = get_n_rnd_element_from_list_excluding_parameter(classes, anchor_class, C - 1) # get all classes different from the query class\n",
    "    for neg_class in negative_classes:\n",
    "        neg_sample = get_n_rnd_element_from_list(os.listdir(os.path.join(src, neg_class)), 1)[0] # select a random sample of the current class\n",
    "        neg_img = read_image(os.path.join(src, neg_class, neg_sample), img_type) \n",
    "        \n",
    "        negatives.append(neg_img)\n",
    "        labels.append(neg_class)\n",
    "        \n",
    "    return anchor_img, positive_img, negatives, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c608f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Validate on evaluation alphabets...\")\n",
    "# one_shot_validate_triplet(model, val_classes, len(val_classes), src, 300, 'RGB')\n",
    "# print(\"Validate on test alphabets...\")\n",
    "# one_shot_validate_triplet(model, test_classes, len(test_classes), src, 300, 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ded65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_train_batch_triplet(src, classes, n_pairs, img_type, batch_size):\n",
    "#     X, labels = [], []\n",
    "\n",
    "#     while(len(X) < batch_size):\n",
    "#         query_class = get_k_rnd_element_from_list(classes, 1)[0]\n",
    "#         query_samples = get_k_rnd_element_from_list(os.listdir(os.path.join(src, query_class)), 2) # select anchor, positive\n",
    "        \n",
    "#         # anchor, positive samplese\n",
    "#         anchor_sample, positive_sample = query_samples[0], query_samples[1]\n",
    "#         anchor_img = read_image(os.path.join(src, query_class, anchor_sample), img_type) \n",
    "#         positive_img = read_image(os.path.join(src, query_class, positive_sample), img_type) \n",
    "\n",
    "#         # negeative sample\n",
    "#         negative_class = get_k_rnd_element_from_list_excluding_parameter(classes, query_class, 1)[0] # get one random class different from the query class\n",
    "#         neg_sample = get_k_rnd_element_from_list(os.listdir(os.path.join(src, negative_class)), 1)[0] # select negative sample\n",
    "#         negative_img = read_image(os.path.join(src, negative_class, neg_sample), img_type) \n",
    "\n",
    "#         X.append([anchor_img, positive_img, negative_img])\n",
    "#         labels.append([query_class, query_class, negative_class])\n",
    "\n",
    "#     y = np.ones(len(X))\n",
    "#     return np.array(X), np.array(y), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d75e5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dimension(s):\n",
    "    ''' Expand the spectrogram dimensions for tensorflow predict_on_batch function use \n",
    "        input s: spectrogram '''\n",
    "    return np.expand_dims(copy.deepcopy(s), axis=0)\n",
    "\n",
    "# modifica al predict similarity qui sopra. non mi interessa avere distanza tr apositivi e negativi perchè \n",
    "## l'embedder che vado ad interrogare è poi sempre lo stesso. faccio in modo da resituire sempre\n",
    "### ap_distance e passare a questa func parametri in modo tale da volere la similarity tra loro due \n",
    "def predict_similarity(f, anchor, s):\n",
    "    X = np.array([[anchor, s, s]])\n",
    "    ap_dist, _ = model([X[:,0], X[:,1], X[:,2]])\n",
    "    return ap_dist.numpy()[0]\n",
    "#     return ap_dist.numpy()[0] if distance_with_positive_sample == True else an_dist.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d74c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology\n",
    "from skimage.filters import sobel\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.segmentation import felzenszwalb, slic, quickshift, watershed\n",
    "from skimage.segmentation import mark_boundaries, find_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "625eff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    ni = None\n",
    "    \n",
    "    if len(img.shape) == 2:\n",
    "        # Two dimensions found. Adding fake channel and stacking it in each RGB dimension.\n",
    "        ni = add_channel_dimension(img)\n",
    "        ni = stack_image(ni)\n",
    "    elif img.shape[2] != 3:\n",
    "        # Third dimension does not have 3 channels. Stacking the first one in each RGB dimension.\n",
    "        ni = stack_image(img)\n",
    "    \n",
    "    return ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b491ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDENTICO STATO ATTUALE GITHUB\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "shapelike_cmap = LinearSegmentedColormap.from_list('shaplike_gradient', (\n",
    "    # Edit this gradient at https://eltos.github.io/gradient/#1E88E5-91C5F2-FFFFFF-FF6395-FF0052\n",
    "    (0.000, (0.118, 0.533, 0.898)),\n",
    "    (0.250, (0.569, 0.773, 0.949)),\n",
    "    (0.500, (1.000, 1.000, 1.000)),\n",
    "    (0.750, (1.000, 0.388, 0.584)),\n",
    "    (1.000, (1.000, 0.000, 0.322)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c00240f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAIHRFWHRUaXRsZQBzaGFwbGlrZV9ncmFkaWVudCBjb2xvcm1hcC7TpnMAAAAmdEVYdERlc2NyaXB0aW9uAHNoYXBsaWtlX2dyYWRpZW50IGNvbG9ybWFwFWBBewAAADB0RVh0QXV0aG9yAE1hdHBsb3RsaWIgdjMuNS4wLCBodHRwczovL21hdHBsb3RsaWIub3JnUjxewQAAADJ0RVh0U29mdHdhcmUATWF0cGxvdGxpYiB2My41LjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmd8msHmAAABtElEQVR4nO3WS27CMABAQacH6XV70V4D0gWwCVgBUdHPm9kEE2PFBqG3vH98rmOMsSzLGGOMt9PlxvX0Ypnef/G88dx6l/F8/ovmTfYxX+/29/Rd8+bnfN/5zn5H1+s+dm6z+8t2H2N2/8l9XK372vO92ued57u7z/X8AMfj6Xo4X4/rZrzz/mH92+ts5//bdbbzzuN13Yx37k8/9+D46XX29vHTz7d5zt/yPOfx5f8HAAgRAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCvgB4swHfpLsiLQAAAABJRU5ErkJggg==\n",
      "text/html": [
       "<div style=\"vertical-align: middle;\"><strong>shaplike_gradient</strong> </div><div class=\"cmap\"><img alt=\"shaplike_gradient colormap\" title=\"shaplike_gradient\" style=\"border: 1px solid #555;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAIHRFWHRUaXRsZQBzaGFwbGlrZV9ncmFkaWVudCBjb2xvcm1hcC7TpnMAAAAmdEVYdERlc2NyaXB0aW9uAHNoYXBsaWtlX2dyYWRpZW50IGNvbG9ybWFwFWBBewAAADB0RVh0QXV0aG9yAE1hdHBsb3RsaWIgdjMuNS4wLCBodHRwczovL21hdHBsb3RsaWIub3JnUjxewQAAADJ0RVh0U29mdHdhcmUATWF0cGxvdGxpYiB2My41LjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmd8msHmAAABtElEQVR4nO3WS27CMABAQacH6XV70V4D0gWwCVgBUdHPm9kEE2PFBqG3vH98rmOMsSzLGGOMt9PlxvX0Ypnef/G88dx6l/F8/ovmTfYxX+/29/Rd8+bnfN/5zn5H1+s+dm6z+8t2H2N2/8l9XK372vO92ued57u7z/X8AMfj6Xo4X4/rZrzz/mH92+ts5//bdbbzzuN13Yx37k8/9+D46XX29vHTz7d5zt/yPOfx5f8HAAgRAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCBAAABAkAAAgSAAAQJAAAIEgAAECQAACAIAEAAEECAACCvgB4swHfpLsiLQAAAABJRU5ErkJggg==\"></div><div style=\"vertical-align: middle; max-width: 514px; display: flex; justify-content: space-between;\"><div style=\"float: left;\"><div title=\"#1e88e5ff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #1e88e5ff;\"></div> under</div><div style=\"margin: 0 auto; display: inline-block;\">bad <div title=\"#00000000\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #00000000;\"></div></div><div style=\"float: right;\">over <div title=\"#ff0052ff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #ff0052ff;\"></div></div>"
      ],
      "text/plain": [
       "<matplotlib.colors.LinearSegmentedColormap at 0x2b11515e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapelike_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd041f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 17})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd6de948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explanation_omniglet(x, S, labels, norm_attributions, probabilities, cmap):\n",
    "    N = len(norm_attributions)\n",
    "    \n",
    "    plt.figure()\n",
    "    f, axarr = plt.subplots(3, N + 1,figsize=(30,20))\n",
    "\n",
    "    axarr[1, 0].imshow(x)\n",
    "    axarr[1, 0].set_axis_off()\n",
    "    axarr[1, 0].set_title('Query x;\\n y = ' + str(labels[0]))\n",
    "\n",
    "    for i in range(N):\n",
    "        sim = round(probabilities[i], 3)\n",
    "    \n",
    "        # THIRD ROW: Normalized attributions\n",
    "#         im3 = axarr[0, i+1].imshow(norm_attributions[i], cmap='coolwarm', interpolation='none', vmin=-1, vmax=1)\n",
    "        im3 = axarr[0, i+1].imshow(np.sum(norm_attributions[i], axis=-1), cmap='coolwarm_r', interpolation='none', vmin=-1, vmax=1)\n",
    "        #         plt.colorbar(im3, ax=axarr[0, i+1])\n",
    "        axarr[0, i+1].set_axis_off()\n",
    "        \n",
    "        \n",
    "        # 4th ROW: Original Inputs\n",
    "        axarr[1, i+1].imshow(S[i])\n",
    "        axarr[1, i+1].set_axis_off()\n",
    "        subscripts = 's' + str(i+1)\n",
    "        axarr[1, i+1].set_title(subscripts + \"; dist = \" + str(sim))\n",
    "        \n",
    "        # 5th row: overlap\n",
    "        axarr[2, i+1].imshow(S[i], alpha=0.7)\n",
    "        axarr[2, i+1].imshow(np.sum(norm_attributions[i], axis=-1), cmap='coolwarm_r', interpolation='none', vmin=-1, vmax=1, alpha=0.7)\n",
    "        axarr[2, i+1].set_axis_off()\n",
    "\n",
    "    # Empty 2 row 1 column element\n",
    "    axarr[0, 0].set_axis_off()\n",
    "    axarr[0, 0].text(0.0, 0.5, '')\n",
    "    axarr[2, 0].set_axis_off()\n",
    "    axarr[2, 0].text(0.0, 0.5, '')\n",
    "    \n",
    "    \n",
    "    f.subplots_adjust(right=0.95)  # making some room for cbar\n",
    "    # getting the lower left (x0,y0) and upper right (x1,y1) corners:\n",
    "    [[x00,y00],[x01,y01]] = axarr[0, N].get_position().get_points()\n",
    "    [[x10,y10],[x11,y11]] = axarr[1, N].get_position().get_points()\n",
    "    \n",
    "    pad = 0.01; width = 0.005\n",
    "    cbar_ax = f.add_axes([x11+pad, y10, width, y01-y10])\n",
    "    axcb = f.colorbar(im3, cax=cbar_ax)\n",
    "    # axcb = f.colorbar(im3, cax=cbar_ax, ticks=[-1, 0, 1])\n",
    "    # axcb.ax.set_yticklabels([str(max_neg), '0', str(max_pos)])  # vertically oriented colorbar\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04c5b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_required = 0 # da inserire in qualche parte in sinex quando si inizializza, dipende dal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "387262b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_input(img_to_segment):\n",
    "    if stack_required:\n",
    "        img_to_segment = preprocess_img(img_to_segment)\n",
    "        \n",
    "    if sobel_required: # Watershed\n",
    "        img_to_segment = sobel(rgb2gray(img_to_segment))\n",
    "        \n",
    "    if mask_required: # MaskSLIC\n",
    "        ar = morphology.remove_small_objects(ar = rgb2gray(img_to_segment) < 0.5, min_size = 200)\n",
    "        mask = morphology.remove_small_holes(ar = ar, area_threshold = 500)\n",
    "        mask = morphology.opening(mask)\n",
    "        aparams['mask'] = mask\n",
    "\n",
    "    R = algo(image=img_to_segment, **aparams) # creating segments on support set si input\n",
    "#     nR = np.unique(R).shape[0] # number of segments\n",
    "    uR = np.unique(R) # unique segments\n",
    "    \n",
    "    return R, uR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa5a30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METODO DIVERSO RISPETTO LO STATO ATTUALE DI SINEX, bisogna vedere se si può uniformare allo stato attuale di sinex\n",
    "def perturb_image_by_disabling(img, segments, segments_to_disable, replace_value = 0):\n",
    "    wc = copy.deepcopy(img) # makes a working deep copy of the image \n",
    "\n",
    "    for seg in segments_to_disable:\n",
    "        pixels = np.where(segments==seg) # finding segment's pixel coordinate\n",
    "        wc[pixels] = replace_value # replacing value\n",
    "    \n",
    "    return wc # sto restituendo solo l'ultimo pixels: WARNING! al momento questo metovo viene usato\n",
    "    # passando solo 1 segmento come parametro\n",
    "    \n",
    "  #  return wc, np.where(wc == replace_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03588e91",
   "metadata": {},
   "source": [
    "qui è diverso da quello vecchio, ho aggiunto l'original_img come parametro da cui prendere i valori della vecchia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8933b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_image_by_enabling(empty_img, segments, segments_to_enable, original_image):\n",
    "    wc = copy.deepcopy(empty_img) # makes a working deep copy of the image \n",
    "    \n",
    "    for seg in segments_to_enable:\n",
    "        pixels = np.where(segments==seg) # finding segment's pixel coordinate\n",
    "        wc[pixels] = original_image[pixels] # replacing value\n",
    "\n",
    "    # return wc, pixels\n",
    "    return wc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537fb836",
   "metadata": {},
   "source": [
    "# SINEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbf13577",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_SEG_VALUE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6f1ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_sinex(f, x, S, verbose=False):\n",
    "    E = []\n",
    "    C = {} # { sidx: {'segments': [...], 'deltas': []} } contribution values map\n",
    "    \n",
    "    for sidx in range(len(S)):\n",
    "        si = S[sidx] # support set si\n",
    "        v = predict_similarity(f, x, si)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Analyzig support set index:\", sidx, \"Predicted distance:\", v)\n",
    "\n",
    "        R, uR = segment_input(si.copy()) # creating segments on support set si input\n",
    "#         print(\"len segments\", len(uR))\n",
    "\n",
    "        \n",
    "        # Initializes current sample contribution\n",
    "        hi = np.full(sh, float(\"-inf\"))\n",
    "        \n",
    "        if verbose:\n",
    "            plt.imshow(si)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            seg_input = mark_boundaries(si, R, mode='thick')\n",
    "            plt.imshow(seg_input) # modifica rispetto al sinex omniglet\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        \n",
    "        C[sidx] = {} # segment -> delta map init\n",
    "        C[sidx]['R'], C[sidx]['contr_seg_map'] = R, []\n",
    "        \n",
    "        \n",
    "        if mask_required == 1:\n",
    "            # REMOVAL FORZATO PER MASKSLIC segmnento 0 è la maschera (o la somma di tutti i segm)\n",
    "            uR = np.delete(uR, np.where(uR == 0))\n",
    "\n",
    "        for seg in uR:\n",
    "            if mode == 1:\n",
    "                zi = perturb_image_by_enabling(np.full(sh, REPLACE_SEG_VALUE), R, [seg], si)\n",
    "            else:\n",
    "                # disabilitare solo current seg \n",
    "                zi = perturb_image_by_disabling(si, R, [seg], [REPLACE_SEG_VALUE,REPLACE_SEG_VALUE,REPLACE_SEG_VALUE])\n",
    "            \n",
    "            idxs = np.where(R == seg)\n",
    "            pxl = si[idxs]\n",
    "            \n",
    "                \n",
    "            u = predict_similarity(f, x, zi) # calculate new similarity\n",
    "            d = v - u # calculate delta of similarity scores\n",
    "            c = d / len(pxl) \n",
    "            hi[idxs] = c\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"New dist:\", u)\n",
    "                print(\"Delta:\", d)\n",
    "                print(\"Contribution:\", c)\n",
    "                print(\"Segment:\", seg)\n",
    "                print(\"Len:\", len(pxl))\n",
    "                plt.imshow(zi)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "                \n",
    "                newR = np.full((224,224), 0)\n",
    "                indexes_newR = np.where(R == seg)\n",
    "                newR[indexes_newR] = R[indexes_newR]\n",
    "                seg_input = mark_boundaries(si, newR, mode='thick')\n",
    "                plt.imshow(seg_input)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "                print(\"-------------------\")\n",
    "\n",
    "            # append contribution value of the current segment \n",
    "            C[sidx]['contr_seg_map'].append((c, seg))\n",
    "                \n",
    "        # Appending current support set attribution's vector\n",
    "        E.append(hi)\n",
    "    \n",
    "    return E, C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a313aed",
   "metadata": {},
   "source": [
    "# SINEXC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb6bc776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments_to_disable_sinexc(curr_seg, drawn_segments):\n",
    "    drawn = drawn_segments.copy()\n",
    "    \n",
    "    if mask_required == 1:\n",
    "         # REMOVAL FORZATO PER MASKSLIC -> seg 0 è la maschera (o la somma di tutit i sgem)\n",
    "        drawn.remove(0)\n",
    "    \n",
    "    \n",
    "    total_segs_to_disable = round((len(drawn) * percentage_of_segments_to_disable) / 100)\n",
    "    \n",
    "#     print(\"Total sample to disable\", total_segs_to_disable)\n",
    "#     print(\"Total drawn segments\", drawn)\n",
    "    \n",
    "    if total_segs_to_disable <= 1:\n",
    "#         lo forzo ad averne 2, in modo tale da sceglierne poi solo 1 oltre il curr seg\n",
    "        total_segs_to_disable = 2\n",
    "    \n",
    "    random_sample = random.sample(drawn, total_segs_to_disable - 1)\n",
    "    random_sample.append(curr_seg)\n",
    "    \n",
    "    return random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e65dd7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_sinexc(f, x, S, verbose=False):\n",
    "    E = []\n",
    "    C = {}\n",
    "    \n",
    "    for sidx in range(len(S)):\n",
    "        si = S[sidx] # support set si\n",
    "\n",
    "        v = predict_similarity(f, x, si) # calculate initial similarity\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Analyzig support set index:\", sidx, \"Predicted distance:\", v)\n",
    "\n",
    "        R, uR = segment_input(si.copy()) # creating segments on support set si input\n",
    "\n",
    "        # Initializes current sample contribution\n",
    "        hi = np.full(sh, float(\"-inf\"))\n",
    "        \n",
    "        if verbose:\n",
    "            seg_input = mark_boundaries(si, R)\n",
    "            plt.imshow(seg_input) # modifica rispetto al sinex omniglet\n",
    "            plt.show()\n",
    "        \n",
    "        C[sidx] = {} # segment -> delta map init\n",
    "        C[sidx]['R'], C[sidx]['contr_seg_map'] = R, []\n",
    "        \n",
    "        if mask_required == 1:\n",
    "            # REMOVAL FORZATO PER MASKSLIC segmnento 0 è la maschera (o la somma di tutti i segm)\n",
    "            uR = np.delete(uR, np.where(uR == 0))\n",
    "\n",
    "        for seg in uR:\n",
    "            sims = 0\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Segment in analysis:\", seg)\n",
    "\n",
    "            for p in range(P):\n",
    "                segs_to_disable = get_segments_to_disable_sinexc(seg, list(uR))\n",
    "\n",
    "                if mode == 1: # curr seg deve rimanere attivo insieme ad un tot di altri\n",
    "                    segs_to_enable = segs_to_disable.copy() # in questo caso sono gli unici che devono rimanere attivi\n",
    "                    zi = perturb_image_by_enabling(np.full(sh, REPLACE_SEG_VALUE), R, segs_to_enable, si)\n",
    "                else: # curr seg deve essere eliminato insieme ad un tot di altri\n",
    "                    zi = perturb_image_by_disabling(si, R, segs_to_disable, [REPLACE_SEG_VALUE,REPLACE_SEG_VALUE,REPLACE_SEG_VALUE])\n",
    "                    \n",
    "                u = predict_similarity(f, x, zi) # calculate new similarity\n",
    "                sims += u\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"Perturbation:\", p, \" for segment:\", seg)\n",
    "                    plt.imshow(zi)\n",
    "                    plt.show()\n",
    "\n",
    "\n",
    "            segidx = np.where(R == seg) # qui uso i segidx e i pixels del segmento corrente in analisi\n",
    "            pixels = si[segidx]\n",
    "\n",
    "            # qui uso i segidx e i pixels del segmento corrente in analisi\n",
    "            d = v - (sims / P) # calculating delta of similarity scores\n",
    "            c = d / len(pixels)\n",
    "            hi[segidx] = c\n",
    "\n",
    "            # append contribution value of the current segment \n",
    "            C[sidx]['contr_seg_map'].append((c, seg))\n",
    "\n",
    "            if verbose:\n",
    "                print(\"New dist:\", u)\n",
    "                print(\"Delta:\", d)\n",
    "                print(\"Segment:\", seg)\n",
    "                plt.imshow(zi)\n",
    "                plt.show()\n",
    "                print(\"-------------------\")\n",
    "    \n",
    "        # Appending current support set attribution's vector\n",
    "        E.append(hi)\n",
    "        \n",
    "    return E, C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90300bf0",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "416b9779",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- WATERSHED -- \n",
    "# algo = watershed\n",
    "# mask_required = 0 # 0: SLIC, 1: MaskSLIC\n",
    "# sobel_required = 1 # da automatizzare in fase di inizializzazione di SINEX\n",
    "\n",
    "# aparams = {\n",
    "#     'markers': 250,\n",
    "#     'compactness': 0.01,\n",
    "# }\n",
    "\n",
    "# -- QUICKSHIFT -- \n",
    "algo = quickshift\n",
    "mask_required = 0 # 0: SLIC, 1: MaskSLIC\n",
    "sobel_required = 0\n",
    "\n",
    "aparams = {\n",
    "    'kernel_size':3, # Width of Gaussian kernel used in smoothing the sample density. Higher means fewer clusters.\n",
    "    'max_dist':60, # Higher means fewer clusters.\n",
    "    'ratio':0.5 # between 0 and 1\n",
    "              ## Balances color-space proximity and image-space proximity. Higher values give more weight to color-space.\n",
    "}\n",
    "\n",
    "\n",
    "# ## -- FELZENSZWALB -- \n",
    "# algo = felzenszwalb\n",
    "# mask_required = 0 # 0: SLIC, 1: MaskSLIC\n",
    "# sobel_required = 0\n",
    "\n",
    "# aparams = {\n",
    "#     'scale':1,  #Higher scale means less and larger segments\n",
    "#     'sigma':0.5, # Width (standard deviation) of Gaussian kernel used in preprocessing.\n",
    "#     'min_size':100 # Higher larger clusters. \n",
    "# }\n",
    "\n",
    "## -- SLIC / MaskSLIC -- \n",
    "# algo = slic\n",
    "# mask_required = 0 # 0: SLIC, 1: MaskSLIC\n",
    "# sobel_required = 0\n",
    "\n",
    "# aparams = {\n",
    "#    'n_segments':15,\n",
    "#     'compactness':100,\n",
    "#     'sigma':1,\n",
    "#     'start_label':1,\n",
    "#     'mask': None\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b72829c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query class 057.Rose_breasted_Grosbeak\n",
      "Positive set class 057.Rose_breasted_Grosbeak - dist. pred: 0.17527652\n",
      "Negative set class 050.Eared_Grebe - dist. pred: 0.8119503\n",
      "Negative set class 140.Summer_Tanager - dist. pred: 0.077079654\n",
      "Negative set class 176.Prairie_Warbler - dist. pred: 4.801177\n",
      "Negative set class 162.Canada_Warbler - dist. pred: 3.4742355\n"
     ]
    }
   ],
   "source": [
    "query_class = '057.Rose_breasted_Grosbeak'\n",
    "x, positive_img, negatives, labels = get_one_shot_batch_validate_triplet(query_class, val_classes, len(val_classes), src, 'RGB')\n",
    "\n",
    "##############################\n",
    "# preparing probabilities array\n",
    "##############################\n",
    "probabilities = []\n",
    "print(\"Query class\", query_class)\n",
    "\n",
    "X = np.array([[x, positive_img, negatives[0]]])\n",
    "ap_distance, an_distance = model([X[:,0], X[:,1], X[:,2]])\n",
    "proba = ap_distance.numpy()[0]\n",
    "probabilities.append(proba)\n",
    "print(\"Positive set class %s - dist. pred: %s\" % (labels[0], proba))\n",
    "\n",
    "for i in range(len(negatives)):\n",
    "    X = np.array([[x, positive_img, negatives[i]]])\n",
    "    ap_distance, an_distance = model([X[:,0], X[:,1], X[:,2]])\n",
    "    proba = an_distance.numpy()[0]\n",
    "    probabilities.append(proba)\n",
    "    print(\"Negative set class %s - dist. pred: %s\" % (labels[i+1], proba))\n",
    "    \n",
    "#####################################    \n",
    "# preparing support set per la stessa firma dell'explainer. in 0 metto il positive_img\n",
    "#####################################\n",
    "S = [positive_img]\n",
    "for el in negatives: S.append(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bccb5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_script(n, label='s'):\n",
    "    if n == 1:\n",
    "        return label + '\\N{SUBSCRIPT ONE}'\n",
    "    elif n == 2:\n",
    "        return label + '\\N{SUBSCRIPT TWO}'\n",
    "    elif n == 3:\n",
    "        return label + '\\N{SUBSCRIPT THREE}'\n",
    "    elif n == 4:\n",
    "        return label + '\\N{SUBSCRIPT FOUR}'\n",
    "    elif n == 5:\n",
    "        return label + '\\N{SUBSCRIPT FIVE}'\n",
    "    else:\n",
    "        return 'null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26f17799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    return list((data - np.min(data)) / (np.max(data) - np.min(data)))\n",
    "\n",
    "def reverse_deletions_insertions(array):\n",
    "    to_return = []\n",
    "    \n",
    "    for x in array:\n",
    "        a = 1.0 - x\n",
    "        to_return.append(a)\n",
    "        \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4632c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_class = '057.Rose_breasted_Grosbeak'\n",
    "x, positive_img, negatives, labels = get_one_shot_batch_validate_triplet(query_class, val_classes, len(val_classes), src, 'RGB')\n",
    "\n",
    "##############################\n",
    "# preparing probabilities array\n",
    "##############################\n",
    "probabilities = []\n",
    "print(\"Query class\", query_class)\n",
    "\n",
    "X = np.array([[x, positive_img, negatives[0]]])\n",
    "ap_distance, an_distance = model([X[:,0], X[:,1], X[:,2]])\n",
    "proba = ap_distance.numpy()[0]\n",
    "probabilities.append(proba)\n",
    "print(\"Positive set class %s - dist. pred: %s\" % (labels[0], proba))\n",
    "\n",
    "for i in range(len(negatives)):\n",
    "    X = np.array([[x, positive_img, negatives[i]]])\n",
    "    ap_distance, an_distance = model([X[:,0], X[:,1], X[:,2]])\n",
    "    proba = an_distance.numpy()[0]\n",
    "    probabilities.append(proba)\n",
    "    print(\"Negative set class %s - dist. pred: %s\" % (labels[i+1], proba))\n",
    "\n",
    "\n",
    "# Storing probabilities from current pos_sample and negative pairs\n",
    "S = [positive_img]\n",
    "for el in negatives: S.append(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINEX MODE 1\n",
    "mode = 2\n",
    "\n",
    "# Storing probabilities from current pos_sample and negative pairs\n",
    "S = [positive_img]\n",
    "for el in negatives: S.append(el)\n",
    "\n",
    "# Get SINEX explanations\n",
    "E, C = explain_sinex(model, x, S, verbose=True)\n",
    "\n",
    "# Normalize attributions\n",
    "# max_pos, max_neg = get_max_positive_max_negative_from_E(E)\n",
    "# NE = normalize_contributions(E, max_pos, max_neg)\n",
    "\n",
    "# # Plot explanation\n",
    "# plot_explanation_omniglet(x, S, labels, NE, probabilities, shapelike_cmap)\n",
    "\n",
    "# # deletion, insertions\n",
    "# dels, ins = get_insertion_and_deletion_scores(x, model, S, C, 0, verbose=False)\n",
    "\n",
    "# # # plot ins/dels curves\n",
    "# plot_curves(dels, ins, 'SINEX Md=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b506bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af422c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
